from cnn.train_cnn_cross import train_cnn_cross
import matplotlib.pyplot as plt
import numpy as np

# Set your fixed hyperparameters
params = dict(
    EPOCHS=40,
    LEARNING_RATE=0.0007202,
    FILTER_LEN=5,
    N_CHANNELS=248,
    N_SOURCES=32,
    DROPOUT=0.488357,
    WEIGHT_DECAY=1e-4,
    PATIENCE=15,
    LR_PATIENCE=7,
    LR_FACTOR=0.1,
    TRAIN_BATCH_SIZE=4,
    TEST_BATCH_SIZE=4,
    DOWNSAMPLE_FACTOR=20,
    NORMALIZE=True,
    RANDOM_SEED=None  # We'll set this in the loop for reproducibility
)

n_runs = 1
val_accs = []
test_accs = []
test_accs_per_subject = []

for i in range(n_runs):
    print(f"\n=== Run {i+1}/{n_runs} ===")
    params['RANDOM_SEED'] = 42 + i  # Change seed for each run
    val_acc, test_accuracies, overall_test_acc, subject_task_accuracy = train_cnn_cross(**params)
    val_accs.append(val_acc)
    test_accs.append(overall_test_acc)
    test_accs_per_subject.append(test_accuracies)
    print(f"Validation accuracy: {val_acc:.2f}%")
    print(f"Test accuracies per subject: {test_accuracies}")
    print(f"Overall test accuracy: {overall_test_acc:.2f}%")

    subjects = list(subject_task_accuracy.keys())
    tasks = sorted({task for subj in subject_task_accuracy.values() for task in subj})

    bar_width = 0.15
    x = np.arange(len(tasks))

    # Optional: pretty display names for tasks
    display_names = {
        "rest": "Rest",
        "task_motor": "Motor",
        "task_story_math": "Math & story",
        "task_working_memory": "Working memory"
    }

    labels = [display_names.get(task, task) for task in tasks]

    plt.figure(figsize=(10, 6))
    for j, subject in enumerate(subjects):
        accs = [subject_task_accuracy[subject].get(task, 0) for task in tasks]
        plt.bar(x + j * bar_width, accs, width=bar_width, label=subject)

    plt.xlabel('Task')
    plt.ylabel('Accuracy (%)')
    plt.title(f'Test Accuracy per Subject per Task - Run {i+1}')
    plt.xticks(x + bar_width * (len(subjects) - 1) / 2, labels)
    plt.ylim(0, 100)
    plt.legend(title='Subject')
    plt.tight_layout()
    plt.savefig(f'subject_task_accuracy_run_{i+1}.png')
    plt.close()

print("\n=== Summary ===")
print(f"Validation accuracies: {val_accs}")
print(f"Overall test accuracies: {test_accs}")
print(f"Test accuracies per subject: {test_accs_per_subject}")
print(f"Mean overall test accuracy: {sum(test_accs)/len(test_accs):.2f}%")

import matplotlib.pyplot as plt

# Plot validation and test accuracies for each run
plt.figure(figsize=(8, 5))
plt.plot(val_accs, marker='o', label='Validation Accuracy')
plt.plot(test_accs, label='Overall Test Accuracy')
plt.xlabel('Run')
plt.ylabel('Accuracy (%)')
plt.title('Validation and Test Accuracies Across Runs')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()